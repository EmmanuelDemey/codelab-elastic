# Step 0

In order to launch the demo app, you should run the following command

```shell
docker-compose up
```

We have many routes :

* GET http://localhost:8080/rest/products
* GET http://localhost:8080/rest/fake/url returning 404
* GET http://localhost:8080/rest/long/task returing 200 after 5s
* GET http://localhost:8080/rest/weather calling an external service

# Step 1

In order to setup Elasticsearch and Kibana in our project, you need to add these extra lines in your `docker-compose.yml` file:

```yml
elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:6.2.3
    ports:
        - 9200:9200
kibana:
    image: docker.elastic.co/kibana/kibana:6.2.3
    depends_on:
        - elasticsearch
    ports:
        - 5601:5601
```

And run the following command:

```shell
docker-compose up
```

* Have a look to the response of GET http://localhost:9200 to check if the cluster is UP
* Open Kibana and show the developer Console
* Index a simple document, and check you retrieve this document in a search:

```
DELETE cars
PUT cars/_doc/1
{
  "model": "c3",
  "manufacturer": "Citroen"
}
# Index many of them
POST cars/_doc
{
  "model": "c3",
  "manufacturer": "Citroen"
}
# Index many of them
POST cars/_doc
{
  "model": "zoe",
  "manufacturer": "Renault"
}
GET cars/_doc/1
GET cars/_search?q=citroen
GET cars/_search
{
  "query": {
    "match": {
      "manufacturer": "citroen"
    }
  }
}
GET cars/_doc/_search
{
  "size": 0,
  "aggs" : {
    "manufacturers_aggs" : { 
      "terms" : { 
        "field" : "manufacturer.keyword"
      }
    }
  }
}
```

* Index 1m documents with:

```sh
# Get the injector if not downloaded yet
wget https://download.elastic.co/workshops/basic-kibana/injector/injector-6.0.jar
# Insert 1m persons randomly generated
java -jar injector-6.0.jar 1000000 10000
```

* Open Kibana and show the basic features:
  * Search
  * Visualizations
  * Dashboard

* Import the pre-built dashboard available in `config/kibana` dir and open it.

# Step 2

We can explain first the content of the `packetbeat.yml` file.

In order to start run:

```shell
docker-compose up
sudo chown root config/packetbeat/packetbeat.yml
sudo ./packetbeat -e -c config/packetbeat/packetbeat.yml
```

* You can now have a look to the HTTP dashboard
  * Explain all graphs
  * Show the filter feature
  * autorefresh
  * date picker

# Step 3

We will now enable Filebeat and index and visualize the logs generated by NGINX. These logs will be indexed wihout any modifications

In order to start you should executed the following commands :

```shell
docker-compose up
sudo chown root config/filebeat/filebeat.yml
sudo ./filebeat -e -c config/filebeat/filebeat.yml
```

* Add the corresponding index into Kibana and have a look to the data
* Create a time serie in order to visualize the number of request by seconds

# Step 4

We will now add Logstash in order to ngrok the log indexed by filebeat

We can first present the grok debugger in Kibana with a very simple sample

```shell
172.18.0.1 the rest of the message
%{IPORHOST:[nginx][access][remote_ip]}
```

In Filebeat, the only thing we need to change is the output. We will send the document to logstash instead of elasticsearch cluster

```shell
docker-compose up
sudo chown root config/filebeat/filebeat.yml
sudo ./filebeat -e -c config/filebeat/filebeat.yml
./logstash -f config/logstash/logstash.conf
```

We will start with an easy logstash configuration file with only the beats input and elasticsearch and stdout outputs

```shell
input { beats { port => "5044" } }
output {
  elasticsearch { hosts => ["localhost:9200"] }
  stdout { codec => rubydebug }
}
```

Then we will add filters :

* grok

```shell
filter {
    grok {
        match => { "message" => "%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \[%{HTTPDATE:[nginx][access][time]}\] \"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"%{DATA:[nginx][access][referrer]}\" \"%{DATA:[nginx][access][agent]}\""}
    }
}
```

* date

```shell
date {
    match => [ "[nginx][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
}  
```

We will finally add a mutate filter in order to delete the original message

```shell
mutate {
    remove_field => [ "message" ]
}
```

* We will debug the result of this configuration file via the stdout output
* Check the data indexed in Elasticsearch
* Show the Grok Debugger

# Step 5

In this section we will reimplement the previous logstash configuration but with the built-in ingest feature.

We will first use the simulate API.

* Ingest a message without any processors

```shell
POST http://localhost:9201/_ingest/pipeline/_simulate
{
"pipeline" : {
"processors": []
},
"docs" : [
{ "\_source": { "message": "172.18.0.1 - - [25/Mar/2018:17:07:43 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\" \"-\"\r\n"} }
]
}
```

* Add the grok processor

```shell
POST http://localhost:9201/_ingest/pipeline/_simulate
{
"pipeline" : {
"processors": [
{
"grok": {
"field": "message",
"patterns": ["%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"%{DATA:[nginx][access][referrer]}\" \"%{DATA:[nginx][access][agent]}\""]
}
}
]
},
"docs" : [
{ "\_source": { "message": "172.18.0.1 - - [25/Mar/2018:17:07:43 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\" \"-\"\r\n"} }
]
}
```

* Add the remove processor

```shell
POST http://localhost:9201/_ingest/pipeline/_simulate
{
"pipeline" : {
"processors": [
{
"grok": {
"field": "message",
"patterns": ["%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"%{DATA:[nginx][access][referrer]}\" \"%{DATA:[nginx][access][agent]}\""]
},
"remove": {
"field": "message"
}
}
]
},
"docs" : [
{ "\_source": { "message": "172.18.0.1 - - [25/Mar/2018:17:07:43 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\" \"-\"\r\n"} }
]
}
```

* Store the pipeline in ES

```shell
PUT http://localhost:9201/_ingest/pipeline/nginx
{
"processors": [
{
"grok": {
"field": "message",
"patterns": ["%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"%{DATA:[nginx][access][referrer]}\" \"%{DATA:[nginx][access][agent]}\""]
},
"remove": {
"field": "message"
}
}
]
}
```

* Use the previous stored pipeline when indexing a new document

```shell
PUT http://localhost:9201/my-index/_doc/my-id?pipeline=nginx
{ "message": "172.18.0.1 - - [25/Mar/2018:17:07:43 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\" \"-\"\r\n"}
```

```shell
GET http://localhost:9201/my-index/_doc/my-id
```

# Step 6

We will add now the last beat of the day : Metricbeat for Docker container

```shell
docker-compose up
sudo chown root config/metricbeat/metricbeat.yml
sudo ./metricbeat -e -c config/metricbeat/metricbeat.yml
```

With the following configuration

```yml
metricbeat.modules:
- module: docker
  metricsets: ["container", "cpu", "diskio", "healthcheck", "info", "memory", "network"]
  hosts: ["unix:///var/run/docker.sock"]
  period: 10s
```

# Step 7

We will now add some Security to our cluster

* First change the ES docker used in order to use the one with XPack enabled
* Define the environnement variable for the password used for ES (used changeme, the one used by default by Kibana)

```shell
environment:
      - ELASTIC_PASSWORD=changeme
```

* Log in to kibana and present the new securty page
* index one fake document that will be used to present fields filtering

```shell
POST devoxx_indices/_doc/1
{
  "firstName": "Manu",
  "secret": "xxxxx"
}
```

* Create a simple dashboard with a saved search

* Create a user manu with the kibana dashboard only role

* Log in with this account and check if we only have access to the dashboard page... But we have no data :(

* Create a role devoxx-reader with read right on the devoxx-reader indice, and remove the secret field of each document

* Check if the role has been created

```shell
GET /_xpack/security/role
```

* Assign this role to the user manu

* Log in again and normally we should have access to the right data

* Add a lastName field to the document. Manu should not be able to see this field

* Get the role thanks to the REST API

'''shell
GET /\_xpack/security/role
'''

* Add the lastName field to the configuration

```shell
POST /_xpack/security/role/devoxx-reader
{
    "cluster": [],
    "indices": [
      {
        "names": [
          "devoxx_indices"
        ],
        "privileges": [
          "read"
        ],
        "field_security": {
          "grant": [
            "firstName",
            "lastName"
          ]
        }
      }
    ],
    "run_as": [],
    "metadata": {},
    "transient_metadata": {
      "enabled": true
    }
  }
```

* Check thanks to the Kibana UI if this modification is enabled

* Check the Kibana UI with Manu

# Step 8 - Alerting

We will now add alerts in our plateform. We will send alerts if we have less than 10 request in the last 5 mn .

* Add username/password to filebeat (kibana option)

'''
setup.kibana:
host: "localhost:5601"
username: "elastic"
password: "changeme"
'''

* Add username/password to Logstash

```shell
elasticsearch {
      hosts => ["localhost:9200"]
      user => elastic
      password => changeme
  }
```

* In the admin part, add the watcher. For the demo, we will only send a log
* Display log for the Elasticsearch cluster

```
docker-compose logs -f elasticsearch
```

* In order to see the payload of a watcher, use first this log and have a look to the logs

```
Votre site n'a pas beaucoup de visiteurs ({{ctx}})
```

* Show the History page of a watcher.

* From the Devtools, execute the following request

```
GET _xpack/watcher/watch/<ID of the watcher>

GET .watcher-history*/_search?pretty
{
  "query": {
            "match": {
              "metadata.name": "devoxx"
            }
          }

}
```

* Create a quick Counter in order to show the number of alerts.

# Step 9 - APM

And what if we monitor our NodeJS server now? We will use the APM feature of the Elastic stack

* Start docker-compose without the back
* Start the back with nodemon in order to have hot reload
* Go the the home page of kibana in order to see how we can enable APM
* Configure the APM server and start it
* Open kibana and check the available dashboards
* Install the NPM dependency
* Add the setup code in the Express serveur
* Show the result in Kibana
* Add a addFilter with a console.log of the payload
* Add nested call to the weather endpoint in order to see multiple spans in a transactions
* Add a custom span with just a simple sleep
